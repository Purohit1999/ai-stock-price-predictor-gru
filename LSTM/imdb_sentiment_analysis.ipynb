{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641b9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25000  Test samples: 25000\n",
      "Epoch 1/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 794ms/step - accuracy: 0.7782 - auc: 0.8643 - loss: 0.4604 - val_accuracy: 0.8494 - val_auc: 0.9212 - val_loss: 0.3609\n",
      "Epoch 2/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 710ms/step - accuracy: 0.8961 - auc: 0.9554 - loss: 0.2671 - val_accuracy: 0.8576 - val_auc: 0.9278 - val_loss: 0.3485\n",
      "Epoch 3/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 720ms/step - accuracy: 0.9361 - auc: 0.9791 - loss: 0.1784 - val_accuracy: 0.8616 - val_auc: 0.9264 - val_loss: 0.4113\n",
      "Quick demo score on a sample review (in-memory model):\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Sample: The movie was fantastic! I absolutely loved it and would watch it again.\n",
      "Score: 0.9083  Prediction: Positive\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMDB Sentiment — Enhanced BiLSTM Trainer & Inference\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Quieter TF logs\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# -----------------------\n",
    "# Reproducibility\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Config (can be overridden via CLI)\n",
    "# -----------------------\n",
    "NUM_WORDS   = 20_000\n",
    "MAXLEN      = 250\n",
    "EMBED_DIM   = 128\n",
    "LSTM_UNITS1 = 128\n",
    "LSTM_UNITS2 = 64\n",
    "DROPOUT     = 0.30\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 10\n",
    "\n",
    "# Works in both scripts and notebooks\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parent\n",
    "except NameError:  # notebook\n",
    "    ROOT = Path.cwd()\n",
    "\n",
    "MODEL_KERAS = ROOT / \"imdb_lstm.keras\"  # preferred\n",
    "MODEL_H5    = ROOT / \"imdb_lstm.h5\"     # legacy\n",
    "\n",
    "# -----------------------\n",
    "# Data loading\n",
    "# -----------------------\n",
    "def load_imdb(num_words: int = NUM_WORDS, maxlen: int = MAXLEN):\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test  = keras.preprocessing.sequence.pad_sequences(x_test,  maxlen=maxlen)\n",
    "    print(f\"Train samples: {len(x_train)}  Test samples: {len(x_test)}\")\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def get_word_index(num_words: int = NUM_WORDS):\n",
    "    \"\"\"IMDB word index, shifted for reserved tokens, limited to top-N.\"\"\"\n",
    "    word_index = keras.datasets.imdb.get_word_index()\n",
    "    word_index = {k: (v + 3) for k, v in word_index.items()}  # shift\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "    return {k: v for k, v in word_index.items() if v < num_words}\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "def build_model(vocab_size: int = NUM_WORDS, maxlen: int = MAXLEN) -> keras.Model:\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    x = layers.Embedding(vocab_size, EMBED_DIM)(inputs)  # input_length deprecated in Keras 3\n",
    "    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS1, return_sequences=True))(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS2))(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"imdb_bilstm\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------------\n",
    "# Training\n",
    "# -----------------------\n",
    "def train_and_save():\n",
    "    (x_train, y_train), (x_test, y_test) = load_imdb()\n",
    "    model = build_model(NUM_WORDS, MAXLEN)\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(MODEL_KERAS),\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-5,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_acc, test_auc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test — loss: {test_loss:.4f}  acc: {test_acc:.4f}  auc: {test_auc:.4f}\")\n",
    "\n",
    "    model.save(MODEL_KERAS)\n",
    "    model.save(MODEL_H5)\n",
    "    print(f\"✅ Saved: {MODEL_KERAS.name} and {MODEL_H5.name}\")\n",
    "\n",
    "    # Optional: extra metrics via scikit-learn\n",
    "    try:\n",
    "        from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "        y_prob = model.predict(x_test, batch_size=512, verbose=0).ravel()\n",
    "        y_pred = (y_prob >= 0.5).astype(\"int32\")\n",
    "        print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    except Exception as e:\n",
    "        print(\"(Extra metrics skipped; install scikit-learn to enable.)\", e)\n",
    "\n",
    "# -----------------------\n",
    "# Inference on custom text\n",
    "# -----------------------\n",
    "_WORD_INDEX_CACHE = None\n",
    "def _simple_clean(s: str) -> List[str]:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s.split()\n",
    "\n",
    "def encode_review(text: str, maxlen: int = MAXLEN) -> np.ndarray:\n",
    "    \"\"\"Encode raw text to the same integer format IMDB dataset uses.\"\"\"\n",
    "    global _WORD_INDEX_CACHE\n",
    "    if _WORD_INDEX_CACHE is None:\n",
    "        _WORD_INDEX_CACHE = get_word_index(NUM_WORDS)\n",
    "    words = _simple_clean(text)\n",
    "    seq = [1]  # <START>\n",
    "    for w in words:\n",
    "        idx = _WORD_INDEX_CACHE.get(w, 2)  # 2 = <UNK>\n",
    "        seq.append(idx)\n",
    "    arr = keras.preprocessing.sequence.pad_sequences([seq], maxlen=maxlen)\n",
    "    return arr\n",
    "\n",
    "def load_model_for_inference() -> keras.Model:\n",
    "    \"\"\"Load a saved model if available; otherwise raise a clear error.\"\"\"\n",
    "    if MODEL_KERAS.exists():\n",
    "        print(f\"📦 Loading {MODEL_KERAS.name}\")\n",
    "        return keras.models.load_model(MODEL_KERAS)\n",
    "    if MODEL_H5.exists():\n",
    "        print(f\"📦 Loading {MODEL_H5.name} (legacy)\")\n",
    "        return keras.models.load_model(MODEL_H5)\n",
    "    raise FileNotFoundError(\n",
    "        f\"No saved model found at:\\n  {MODEL_KERAS}\\n  {MODEL_H5}\\n\"\n",
    "        \"Run this script with --train at least once to create them.\"\n",
    "    )\n",
    "\n",
    "def predict_text(text: str) -> Tuple[float, str]:\n",
    "    model = load_model_for_inference()\n",
    "    x = encode_review(text)\n",
    "    prob = float(model.predict(x, verbose=0).ravel()[0])\n",
    "    label = \"Positive\" if prob >= 0.5 else \"Negative\"\n",
    "    return prob, label\n",
    "\n",
    "# -----------------------\n",
    "# CLI (robust in Jupyter)\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"IMDB Sentiment — BiLSTM\")\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", help=\"Train the model\")\n",
    "    parser.add_argument(\"--predict\", type=str, help=\"Score a custom review\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=EPOCHS, help=\"Override epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=BATCH_SIZE, help=\"Override batch size\")\n",
    "\n",
    "    # Ignore unknown args that Jupyter injects (e.g., -f <path>)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Allow simple overrides\n",
    "    EPOCHS = int(args.epochs)\n",
    "    BATCH_SIZE = int(args.batch_size)\n",
    "\n",
    "    if args.train:\n",
    "        train_and_save()\n",
    "\n",
    "    elif args.predict is not None:\n",
    "        p, lab = predict_text(args.predict)\n",
    "        print(f\"Review: {args.predict}\")\n",
    "        print(f\"Score: {p:.4f}  Prediction: {lab}\")\n",
    "\n",
    "    else:\n",
    "        # ---- Quick demo that DOES NOT require saved files ----\n",
    "        (x_train, y_train), _ = load_imdb()\n",
    "        model = build_model()\n",
    "        model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=3,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            verbose=1,\n",
    "        )\n",
    "        print(\"Quick demo score on a sample review (in-memory model):\")\n",
    "        sample = \"The movie was fantastic! I absolutely loved it and would watch it again.\"\n",
    "        x = encode_review(sample)\n",
    "        prob = float(model.predict(x, verbose=0).ravel()[0])\n",
    "        lab = \"Positive\" if prob >= 0.5 else \"Negative\"\n",
    "        print(f\"Sample: {sample}\\nScore: {prob:.4f}  Prediction: {lab}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
